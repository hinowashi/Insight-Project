{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Project --Birding Big Year--\n",
    "\n",
    "In this project I intend to determine a way to see all the birds one can see on a single state, for a given time window.  For all those birdirers that want to get to the top 100 of their state on ebrid, this will be the perfect tool. The user will input the state, home address (or lat,lon), time window and birds that already have been seen*. This last one (*) is an optional thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "import struct\n",
    "import pickle\n",
    "import googlemaps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize, read_json\n",
    "\n",
    "def save_fig(name):\n",
    "    fig.savefig(name,dpi=80,bbox_inches='tight', pad_inches=0.02, format = 'png')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ebird Data\n",
    "\n",
    "I will start with a singe state. Since the ebird API limits the type of request I can make, I have a downloaded the cvs file.  I'm using the last two full years of data but in reality the alorithm should be train with more data and just tested on the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfAll = pd.read_csv('./ebd_US-WY_201801_201912_relApr-2020/ebd_US-WY_201801_201912_relApr-2020.txt'\n",
    "#                 ,delimiter=\"\\t\")\n",
    "\n",
    "dfAll = pd.read_csv('./ebd_US-WI_201801_201912_relApr-2020/ebd_US-WI_201801_201912_relApr-2020.txt'\n",
    "                ,delimiter=\"\\t\", usecols=['CATEGORY', 'LOCALITY TYPE', 'ALL SPECIES REPORTED', 'APPROVED',\n",
    "                                         'SAMPLING EVENT IDENTIFIER', 'COMMON NAME', 'LOCALITY', \n",
    "                                          'LATITUDE', 'LONGITUDE',\n",
    "                                          'OBSERVATION DATE', 'ALL SPECIES REPORTED'])\n",
    "\n",
    "# dfAll = pd.read_csv('./ebd_US-WI_201001_201812_relApr-2020/ebd_US-WI_201001_201812_relApr-2020.txt'\n",
    "#                 ,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add sertain condition to satify completnes fo the data, public locations and only bird species (i.e. no hybirds). `dfReduce` will contian all the information I will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = dfAll[(dfAll['CATEGORY'] == 'species') & (dfAll['LOCALITY TYPE'] == 'H')\n",
    "              & (dfAll['ALL SPECIES REPORTED'] == 1)  & (dfAll['APPROVED'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReduce = dfAll.filter(['SAMPLING EVENT IDENTIFIER', 'COMMON NAME', 'LOCALITY',\n",
    "              'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE', 'ALL SPECIES REPORTED']) \n",
    "dfReduce['OBSERVATION DATE'] = pd.to_datetime(dfReduce['OBSERVATION DATE'])\n",
    "dfReduce['YEAR WEEK'] = dfReduce['OBSERVATION DATE'].dt.strftime('%W')\n",
    "dfReduce['YEAR DAY'] = dfReduce['OBSERVATION DATE'].dt.strftime('%j')\n",
    "dfReduce['YEAR'] = dfReduce['OBSERVATION DATE'].dt.strftime('%Y')\n",
    "dfReduce['YEAR WEEK'] = pd.to_numeric(dfReduce['YEAR WEEK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReduce.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfReduce contains both my train set and my validation set.  In this case I will use the last year as my validation set (2019) and all the previous information as my train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValidation = dfReduce[dfReduce['YEAR']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = dfReduce[dfReduce['YEAR']!=2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfReduce\n",
    "del dfAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(dfTrain['LOCALITY'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering using BDSCAN\n",
    "\n",
    "BDSCAN is a density clustering that will tell where is popular for people to go birding (based on the desnity of hotsopts).  I will define a cluster as having atleast 3 point and with a maximum distance of 0.05degrees or about 5km.  With that I will optain where does each hotspot ('LOCALITY') belongs to. If '-1' they are not part of any cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbdscan = dfTrain.filter(['LOCALITY','LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbdscan.drop_duplicates(subset='LATITUDE', keep = 'first', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbdscan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latHotspot, lonHotspot = np.array(dfbdscan['LATITUDE']), np.array(dfbdscan['LONGITUDE'])\n",
    "locationList = np.empty((latHotspot.shape[0],2))\n",
    "for i in range(0, dfbdscan.shape[0]):\n",
    "    locationList[i,0], locationList[i,1] = latHotspot[i], lonHotspot[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = .05\n",
    "dbmod = DBSCAN(eps=eps, min_samples=3, metric='euclidean').fit(locationList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbmod.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(dbmod, open(\"./bdclusterModel.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dbmod.labels_\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "core_samples_mask = np.zeros_like(dbmod.labels_, dtype=bool)\n",
    "core_samples_mask[dbmod.core_sample_indices_] = True\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set(labels)\n",
    "colors = [plt.cm.viridis_r(each)\n",
    "          for each in np.linspace(0, 1, len(unique_labels))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, clust in enumerate(labels):\n",
    "    plt.scatter(locationList[i][1],locationList[i][0], color = colors[clust])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbdscan['BD CLUSTER'] = dbmod.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfbdscan.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hotspot_finder_for_dbcluster(k, df, dfbdscan):\n",
    "    dfIntermediate = dfTrain[dfTrain['LOCALITY'].isin(list(dfbdscan[dfbdscan['BD CLUSTER'] == k]['LOCALITY']))].groupby(['LOCALITY','COMMON NAME']).sum().filter(['ALL SPECIES REPORTED'])\n",
    "    dfIntermediate['BIRD'] = list(map(lambda x: 1 if x < 1e6 else 1, dfIntermediate['ALL SPECIES REPORTED']))\n",
    "    location  = dfIntermediate.filter(['BIRD']).reset_index().groupby(['LOCALITY']).sum().reset_index().sort_values(by='BIRD', ascending=False).reset_index()['LOCALITY'][0]\n",
    "    lat = df[df['LOCALITY']==location].reset_index()['LATITUDE'][0]\n",
    "    lon = df[df['LOCALITY']==location].reset_index()['LONGITUDE'][0]\n",
    "    \n",
    "    return location, lat, lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coorHotspot = np.empty((n_clusters_, 2))\n",
    "for i in range(0, n_clusters_):\n",
    "    a, coorHotspot[i, 0], coorHotspot[i, 1] = hotspot_finder_for_dbcluster(i, dfTrain, dfbdscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now some good plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = gpd.read_file('/Users/casanova/DocumentsHere/Insight/gz_2010_us_040_00_5m.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = 'Wyoming'\n",
    "state = 'Wisconsin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(7,8))\n",
    "base = country[country['NAME'].isin([state]) == True].plot(ax=ax, color='#3B3C6E', alpha = 0.3)\n",
    "for i, clust in enumerate(labels):\n",
    "    ax.scatter(locationList[i][1],locationList[i][0], color = colors[clust])\n",
    "ax.scatter(coorHotspot[:,1],coorHotspot[:,0], marker = 'x', color = 'r', s=80)\n",
    "ax.set_ylabel(r'Latitude [$^o$]')\n",
    "ax.set_xlabel(r'Longitude [$^o$]')\n",
    "\n",
    "plt.show()\n",
    "save_fig('/Users/casanova/DocumentsHere/Insight/{}-plain.png'.format(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the bird probability.\n",
    "\n",
    "`dfbdscan` have the information of where each of the hotspots lay, in terms of their cluster.  Now in order to constuct a path is important to mask the probabilites of the of seeing a particular bird with T or F on a weekly basis.  This is critical in order to construc the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProb = dfTrain.merge(dfbdscan.filter(['LOCALITY','BD CLUSTER']),\n",
    "                            left_on='LOCALITY', right_on='LOCALITY', how = 'left').filter(['COMMON NAME','ALL SPECIES REPORTED','YEAR WEEK', 'BD CLUSTER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTime = 54\n",
    "nLoc = n_clusters_\n",
    "setMat = np.empty((nTime,nLoc), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for week in range(0,nTime):\n",
    "    dfProbA = dfProb[dfProb['YEAR WEEK']== week]\n",
    "    dfProb1 = dfProbA.groupby(['COMMON NAME','BD CLUSTER']).sum().filter(['ALL SPECIES REPORTED']).reset_index()\n",
    "    dfProb1.rename(columns = {'ALL SPECIES REPORTED':'POS OBS'}, inplace=True)\n",
    "    dfProb2 = dfProbA.groupby(['BD CLUSTER']).sum().filter(['ALL SPECIES REPORTED']).reset_index()\n",
    "    dfProb2.rename(columns = {'ALL SPECIES REPORTED':'TOT OBS'}, inplace=True)\n",
    "    dfProb3 = dfProb1.merge(dfProb2, left_on='BD CLUSTER', right_on='BD CLUSTER', how = 'left')\n",
    "    dfProb3['POS PROB'] = dfProb3['POS OBS']/dfProb3['TOT OBS']\n",
    "    for loc in range(0,nLoc):\n",
    "        dfWeek = dfProb3[dfProb3['BD CLUSTER'] == loc]\n",
    "        dfWeek['TF aa'] = list(map(lambda x: 0 if x < 0.02 else 1, dfWeek['POS PROB']))\n",
    "        setMat[week,loc] = set(dfWeek[dfWeek['TF aa'] == 1]['COMMON NAME'].values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(setMat, open(\"./2dSetLocations.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToMakeUniverse = list(setMat.flatten())\n",
    "Universe = set(e for s in ToMakeUniverse for e in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we go!!!!!\n",
    "\n",
    "First user inputs some coordinates.\n",
    "Then the coordinates get translated to a cluster.\n",
    "That give us the first set (first week)\n",
    "Then we obtain the rest of the sets. The key here is to back track a set to an actual 'x,t' entry so we can have a route.\n",
    "Display in some way that list of locations!  (Probabily using the centroid maps or coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# userInputLat,userInputLon = 44, -110\n",
    "userInputLat,userInputLon = 43.069511, -89.396723\n",
    "\n",
    "userInput = [userInputLat,userInputLon]\n",
    "print(userInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first week I most see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeansLoaded = pickle.load(open(\"./bdclusterModel.p\", \"rb\" ))\n",
    "setMatLoaded = pickle.load(open(\"./2dSetLocations.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print('you start in location:',kmeansLoaded.predict([userInput])[0])\n",
    "# initialLocSet = setMat[0,kmeansLoaded.predict([userInput])[0]]\n",
    "# print(list(initialLocSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hole list of bird that we are planing to see are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ToMakeUniverse = list(setMatLoaded.flatten())\n",
    "Universe = set(e for s in ToMakeUniverse for e in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('With a total of', len(list(Universe)), 'birds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cover_mine(elements, subsets):\n",
    "    '''\n",
    "    There is a greedy algorithm for polynomial time approximation of set covering that chooses sets according to one rule: at each stage, choose the set that contains the largest number of uncovered elements.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    covered = set() \n",
    "    cover = []\n",
    "    listCover = []\n",
    "    # Greedily add the subsets with the most uncovered points\n",
    "    while covered != elements:\n",
    "        subset = max(subsets, key=lambda s: len(s - covered))\n",
    "        cover.append(subset)\n",
    "        listCover.append(subsets.index(subset))\n",
    "        covered |= subset\n",
    " \n",
    "    return cover, listCover\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setList, locList = set_cover_mine(Universe, ToMakeUniverse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locList = np.sort(locList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTime, nLoc = setMatLoaded.shape\n",
    "locMat = np.linspace(1,nTime*nLoc,nTime*nLoc).reshape(nTime,nLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outList = []\n",
    "for element in locList:\n",
    "    a,b = np.where(locMat == element)\n",
    "    outList.append('On week {}, you need to be at location {}'.format(a[0],b[0]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SetCover\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [set([1,2]), \n",
    "     set([1]), \n",
    "     set([1,2,3]), \n",
    "     set([1]), \n",
    "     set([3,4]), \n",
    "     set([4]), \n",
    "     set([1,2]), \n",
    "     set([3,4]), \n",
    "     set([1,2,4])]\n",
    "\n",
    "C = [1, 1, 2, 2, 2, 3, 3, 4, 4]\n",
    "\n",
    "U = set([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3, 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SetCover.universe_maker(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{1, 2, 3}, {3, 4}], [2, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SetCover.set_cover_greedy(U,S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2, 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(S, key=lambda s: len(s - set([1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.4444444444444446, 1000000.0, 2.4444444444444446, 1000000.0, 2.4444444444444446, 0.814814814814815, 0.814814814814815, 1.2222222222222223, 1.2222222222222223]\n",
      "5\n",
      "{4}\n"
     ]
    }
   ],
   "source": [
    "C0 = C/np.mean(C)\n",
    "a = list(map(lambda x, y: len(x -set([1]))/y if len(x -set([1])) != 0 else 1e6, S, C0))\n",
    "print(a)\n",
    "print(a.index(min(a)))\n",
    "print(S[a.index(min(a))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-073f30421510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSetCover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_cover_weighted_greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/DocumentsHere/Insight/SetCover.py\u001b[0m in \u001b[0;36mset_cover_weighted_greedy\u001b[0;34m(universe, setsList, weightList)\u001b[0m\n\u001b[1;32m     70\u001b[0m    \u001b[0mcover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m    \u001b[0mlistCover\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m    \u001b[0mweightList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweightList\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweightList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m    \u001b[0;32mwhile\u001b[0m \u001b[0mcovered\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0muniverse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m        \u001b[0msetWeightStep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcovered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0my\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcovered\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1e6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetsList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweightList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "SetCover.set_cover_weighted_greedy(U,S,C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plaing with google distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmaps = googlemaps.Client(key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode_result = gmaps.geocode('1600 Amphitheatre Parkway, Mountain View, CA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geocode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_geocode_result = gmaps.reverse_geocode((40.714224, -73.961452))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_geocode_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanceMatGmaps = gmaps.distance_matrix(origins = locationList, \n",
    "                                         destinations=locationList, \n",
    "                                         mode = 'driving', \n",
    "                                         units = 'metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = gmaps.distance_matrix(origins = [(latHotspot[0], lonHotspot[0]),\n",
    "                                       (latHotspot[1], lonHotspot[1]),\n",
    "                                       (latHotspot[2], lonHotspot[2]),\n",
    "                                       (latHotspot[3], lonHotspot[3]),\n",
    "                                       (latHotspot[4], lonHotspot[4]),\n",
    "                                       (latHotspot[5], lonHotspot[5]),\n",
    "                                       (latHotspot[6], lonHotspot[6]),\n",
    "                                       (latHotspot[7], lonHotspot[7]),\n",
    "                                       (latHotspot[8], lonHotspot[8]),\n",
    "                                       (latHotspot[9], lonHotspot[9])],\n",
    "                          destinations=[(latHotspot[0], lonHotspot[0]),\n",
    "                                       (latHotspot[1], lonHotspot[1]),\n",
    "                                       (latHotspot[2], lonHotspot[2]),\n",
    "                                       (latHotspot[3], lonHotspot[3]),\n",
    "                                       (latHotspot[4], lonHotspot[4]),\n",
    "                                       (latHotspot[5], lonHotspot[5]),\n",
    "                                       (latHotspot[6], lonHotspot[6]),\n",
    "                                       (latHotspot[7], lonHotspot[7]),\n",
    "                                       (latHotspot[8], lonHotspot[8]),\n",
    "                                       (latHotspot[9], lonHotspot[9])],\n",
    "                         mode = 'driving', units = 'metric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b['rows'][1]['elements'][2]['duration']['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distMat = np.empty((10,10))\n",
    "\n",
    "for i in range(0,10):\n",
    "    for j in range(0,10):\n",
    "        distMat[i,j] = b['rows'][i]['elements'][j]['duration']['value']/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "15391/3600"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
