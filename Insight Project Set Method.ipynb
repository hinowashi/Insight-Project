{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insight Project --Birding Big Year--\n",
    "\n",
    "In this project I intend to determine a way to win the Big Year competition by the American Birding Association (ABA), following their rules. As part of their rules they give the list of eligible birds (1116).  All the birds have to be seen with in 12:00 AM, January 1st to 11:59 PM, December 31st of the same year. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "import struct\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize, read_json\n",
    "\n",
    "def save_fig(name):\n",
    "    fig.savefig(name,dpi=80,bbox_inches='tight', pad_inches=0.02, format = 'jpg')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The ebird Data\n",
    "\n",
    "I will start with a singe state the state of WY. Since the ebird API limits the type of request I can make, I have a downloaded the cvs file.  I'm using the last two full years of data but in reality the alorithm should be train with more data and just tested on the last year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = pd.read_csv('./ebd_US-WY_201801_201912_relApr-2020/ebd_US-WY_201801_201912_relApr-2020.txt'\n",
    "                ,delimiter=\"\\t\")\n",
    "\n",
    "# dfAll = pd.read_csv('./ebd_US-WI_201001_201812_relApr-2020/ebd_US-WI_201001_201812_relApr-2020.txt'\n",
    "#                 ,delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I add sertain condition to satify completnes fo the data, public locations and only bird species (i.e. no hybirds). `dfReduce` will contian all the information I will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll = dfAll[(dfAll['CATEGORY'] == 'species') & (dfAll['LOCALITY TYPE'] == 'H')\n",
    "              & (dfAll['ALL SPECIES REPORTED'] == 1)  & (dfAll['APPROVED'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReduce = dfAll.filter(['SAMPLING EVENT IDENTIFIER', 'COMMON NAME', 'LOCALITY', 'TIME OBSERVATIONS STARTED',\n",
    "              'LATITUDE', 'LONGITUDE', 'OBSERVATION DATE', 'ALL SPECIES REPORTED']) \n",
    "dfReduce['OBSERVATION DATE'] = pd.to_datetime(dfReduce['OBSERVATION DATE'])\n",
    "dfReduce['YEAR WEEK'] = dfReduce['OBSERVATION DATE'].dt.strftime('%W')\n",
    "dfReduce['YEAR DAY'] = dfReduce['OBSERVATION DATE'].dt.strftime('%j')\n",
    "dfReduce['YEAR'] = dfReduce['OBSERVATION DATE'].dt.strftime('%Y')\n",
    "dfReduce['YEAR WEEK'] = pd.to_numeric(dfReduce['YEAR WEEK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReduce.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dfReduce contains both my train set and my validation set.  In this case I will use the last year as my validation set (2019) and all the previous information as my train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfValidation = dfReduce[dfReduce['YEAR']==2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain = dfReduce[dfReduce['YEAR']!=2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let do the k-mean clustering\n",
    "\n",
    "From `dfTrain` data using a k-mean clustering I will select the clusters that will be use on the path finder. This clusters are fixed in space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(init='k-means++', n_clusters=11, n_init=10,random_state = 2345)\n",
    "dfKMeans = dfTrain.filter(['LATITUDE', 'LONGITUDE', 'LOCALITY']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(dfKMeans.filter(['LATITUDE', 'LONGITUDE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = dfTrain.filter(['LATITUDE', 'LONGITUDE']).drop_duplicates()\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = np.min(plotter['LATITUDE']),  np.max(plotter['LATITUDE'])\n",
    "y_min, y_max = np.min(plotter['LONGITUDE']), np.max(plotter['LONGITUDE'])\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower', alpha = 0.5)\n",
    "plt.scatter(plotter['LATITUDE'],plotter['LONGITUDE'], marker = '+')\n",
    "plt.scatter(centroids[:,0], centroids[:,1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.predict([[42.024710,-110.589578]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfKMeans['K-cluster'] = np.array(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfKMeans.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the bird probability.\n",
    "\n",
    "`dfKMeans` has the information of where each of the hotspots lay, in terms of their cluster.  Now in order to constuct a path is important to mask the probabilites of the of seeing a particular bird with T or F on a weekly basis.  This is critical in order to construc the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProb = dfTrain.merge(dfKMeans.filter(['LOCALITY','K-cluster']),\n",
    "                            left_on='LOCALITY', right_on='LOCALITY', how = 'left').filter(['COMMON NAME','ALL SPECIES REPORTED','YEAR WEEK', 'K-cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfProb.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTime = 54\n",
    "nLoc = dfKMeans['K-cluster'].unique().shape[0]\n",
    "setMat = np.zeros((nTime,nLoc), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for week in range(0,nTime):\n",
    "    dfProbA = dfProb[dfProb['YEAR WEEK']== week]\n",
    "    dfProb1 = dfProbA.groupby(['COMMON NAME','K-cluster']).sum().filter(['ALL SPECIES REPORTED']).reset_index()\n",
    "    dfProb1.rename(columns = {'ALL SPECIES REPORTED':'POS OBS'}, inplace=True)\n",
    "    dfProb2 = dfProbA.groupby(['K-cluster']).sum().filter(['ALL SPECIES REPORTED']).reset_index()\n",
    "    dfProb2.rename(columns = {'ALL SPECIES REPORTED':'TOT OBS'}, inplace=True)\n",
    "    dfProb3 = dfProb1.merge(dfProb2, left_on='K-cluster', right_on='K-cluster', how = 'left')\n",
    "    dfProb3['POS PROB'] = dfProb3['POS OBS']/dfProb3['TOT OBS']\n",
    "    for loc in range(0,nLoc):\n",
    "        aa = dfProb3[dfProb3['K-cluster'] == loc]\n",
    "        aa['TF aa'] = list(map(lambda x: 0 if x < 0.02 else 1, aa['POS PROB']))\n",
    "        setMat[week,loc] = set(aa[aa['TF aa'] == 1]['COMMON NAME'].values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ToMakeUniverse = list(setMat.flatten())\n",
    "Universe = set(e for s in ToMakeUniverse for e in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(Universe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here we go!!!!!\n",
    "\n",
    "First user inputs some coordinates.\n",
    "Then the coordinates get translated to a k-cluster.\n",
    "That give us the first set (first week)\n",
    "Then we obtain the resto fo the sets. The key here is to back track a set to an actual 'x,t' entry so we can have a route.\n",
    "Display in some way that list of locations!  (Probabily using the centroid maps or coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInputLat,userInputLon = 44, -110\n",
    "userInput = [userInputLat,userInputLon]\n",
    "print(userInput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the first week I most see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initialLocSet = setMat[0,kmeans.predict([userInput])[0]]\n",
    "print(list(initialLocSet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hole list of bird that we are planing to see are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(list(Universe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('With a total of', len(list(Universe)), 'birds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cover_mine(elements, subsets, initset):\n",
    "    '''\n",
    "    There is a greedy algorithm for polynomial time approximation of set covering that chooses sets according to one rule: at each stage, choose the set that contains the largest number of uncovered elements.\n",
    "\n",
    "    \n",
    "    '''\n",
    "    covered = initset.copy()  \n",
    "    cover = []\n",
    "    listCover = []\n",
    "    # Greedily add the subsets with the most uncovered points\n",
    "    while covered != elements:\n",
    "        subset = max(subsets, key=lambda s: len(s - covered))\n",
    "        cover.append(subset)\n",
    "        listCover.append(subsets.index(subset))\n",
    "        covered |= subset\n",
    " \n",
    "    return cover, listCover\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aa, bb = set_cover_mine(Universe, ToMakeUniverse, initialLocSet)\n",
    "print(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbb = np.sort(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locMat = np.linspace(1,nTime*nLoc,nTime*nLoc).reshape(nTime,nLoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for element in bbb:\n",
    "    a,b = np.where(locMat == element)\n",
    "    print('On week:',a[0],'You need to be at location:',b[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
